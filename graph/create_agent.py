from langgraph.prebuilt import create_react_agent
from langchain_core.runnables import RunnableLambda

from .state import BrickState
from .llm import get_llm_by_type
from .agent import AGENT_LLM_MAP
from .load_template import apply_prompt_template

from langchain_core.messages import AIMessage
from typing import Dict, Any
from langchain_core.output_parsers import PydanticOutputParser

def create_agent(agent_name: str, agent_type: str, tools: list, prompt_template_name: str, state_dict: Dict[str, Any]):
    """Factory function to create agents with consistent configuration."""

    #output_parser = PydanticOutputParser(pydantic_object=EnvCheckerOutput)
    prompt = apply_prompt_template(prompt_template_name, state_dict)
    #print("prompt: ",prompt)
    react_agent_core = create_react_agent(
        name=agent_name,
        model=get_llm_by_type(AGENT_LLM_MAP[agent_type]),
        tools=tools,
        #prompt=lambda x: (print("Prompt state:", x) or x.get("messages", [])),
        prompt=prompt
    )
    return react_agent_core
'''
    def _prepare_agent_input(state_dict: Dict[str, Any]) -> Dict[str, Any]:
        """
        è¿™ä¸ªå‡½æ•°æ¥æ”¶å®Œæ•´çš„ AgentState å­—å…¸ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸º react_agent_core æ‰€éœ€çš„è¾“å…¥æ ¼å¼ã€‚
        å®ƒä¼šè°ƒç”¨ apply_prompt_template æ¥ç”ŸæˆåŒ…å« System Prompt çš„æ¶ˆæ¯åˆ—è¡¨ã€‚
        """
        print("RunnableLambda input_dict (from _prepare_agent_input):", state_dict) # æ‰“å° LangGraph ä¼ é€’çš„å®Œæ•´çŠ¶æ€
        
        rendered_messages_list = apply_prompt_template(prompt_template_name, state_dict)
        print("apply_prompt_template return value (rendered messages list):", rendered_messages_list) # æ‰“å° apply_prompt_template çš„è¿”å›å€¼

        rendered_messages_list = apply_prompt_template(prompt_template_name, state_dict)


        return {
            "messages": rendered_messages_list,
            **state_dict
        }

    def _extract_final_ai_message_content(agent_output: Dict[str, Any]) -> str:
        """
        ä» react_agent_core çš„è¾“å‡ºå­—å…¸ä¸­æå–æœ€åä¸€ä¸ª AIMessage çš„å†…å®¹ã€‚
        è¿™ä¸ªå†…å®¹åº”è¯¥æ˜¯ LLM æŒ‰ç…§ Pydantic æ ¼å¼æŒ‡ä»¤ç”Ÿæˆçš„ JSON å­—ç¬¦ä¸²ã€‚
        """
        print("Extracting final AI message content from agent_output:", agent_output)
        messages = agent_output.get('messages', [])
        
        # éå†æ¶ˆæ¯åˆ—è¡¨ï¼Œæ‰¾åˆ°æœ€åä¸€ä¸ª AIMessage
        for msg in reversed(messages):
            if isinstance(msg, AIMessage):
                print("return msg content: ",msg.content)
                return msg.content
        
        print("Warning: No AIMessage found in agent_output for content extraction.")
        return "" 

    # === æ„å»ºå®Œæ•´çš„ Runnable é“¾ ===
    full_agent_runnable = (
        RunnableLambda(_prepare_agent_input) # 1. å‡†å¤‡ LLM è¾“å…¥
        | react_agent_core                    # 2. è¿è¡Œ ReAct ä»£ç†ï¼Œè¾“å‡ºä¸€ä¸ªå¤æ‚å­—å…¸ (åŒ…å« messages, tool_calls, intermediate_stepsç­‰)
        #| RunnableLambda(_extract_final_ai_message_content) # 3. ä»å¤æ‚å­—å…¸ä¸­æå–æœ€ç»ˆçš„ LLM æ–‡æœ¬å†…å®¹ï¼ˆæœŸæœ›æ˜¯ JSON å­—ç¬¦ä¸²ï¼‰
        #| output_parser             # 4. PydanticOutputParser æ¥æ”¶å­—ç¬¦ä¸²å¹¶å°è¯•è§£æä¸º EnvCheckerOutput
    )

    return full_agent_runnable'''

def create_agent3(agent_name: str, agent_type: str, tools: list, prompt_template_name: str):
    """Factory function to create agents with consistent configuration."""

    react_agent_core = create_react_agent(
        name=agent_name,
        model=get_llm_by_type(AGENT_LLM_MAP[agent_type]),
        tools=tools,
        prompt=lambda x: (print("prompt state:",x) or x.get("messages", [])), 
    )

    full_agent_runnable = (
        RunnableLambda(lambda state_dict: (
            print("RunnableLambda input_dict:", state_dict) or
                {
                    "messages": apply_prompt_template(prompt_template_name, state_dict),
                    **state_dict 
                }
            )
        )
        | react_agent_core 
    )

    return full_agent_runnable

def create_agent2(agent_name: str, agent_type: str, tools: list, prompt_template: str):
    """Factory function to create agents with consistent configuration."""
    #print(get_llm_by_type(AGENT_LLM_MAP[agent_type]))
    print("create agent", agent_name)
    agent = create_react_agent(
        name=agent_name,
        model=get_llm_by_type(AGENT_LLM_MAP[agent_type]),
        tools=tools,
        #prompt=lambda state: apply_prompt_template(prompt_template, state),
        prompt=lambda state: (
            print("Prompt state:", state) or
            apply_prompt_template(prompt_template, state)
        ),
    )
    #return RunnableLambda(lambda input_dict: {**input_dict, "messages": input_dict.get("messages", []), "question": input_dict.get("question", ""), "data_info": input_dict.get("data_info", "")}) | agent
    return RunnableLambda(
        lambda input_dict: (
                print("RunnableLambda input_dict:", input_dict) or
                {**input_dict,
                "messages": input_dict.get("messages", []),
                "question": input_dict.get("question", ""),
                "data_info": input_dict.get("data_info", "")}
            )
    ) | agent

'''
def extract_state_from_messages(messages):
    merged_state = {}
    for msg in messages:
        # ä»æ¶ˆæ¯çš„ additional_kwargs ä¸­æŸ¥æ‰¾ state_data
        state_data = None
        if hasattr(msg, "additional_kwargs") and isinstance(msg.additional_kwargs, dict):
            state_data = msg.additional_kwargs.get("state_data", None)
        if state_data:
            if isinstance(state_data, dict):
                merged_state.update(state_data)
            elif isinstance(state_data, AgentState):
                merged_state.update(state_data.dict())
    return merged_state
    


def create_agent(agent_name: str, agent_type: str, tools: list, prompt_template: str):
    print("create agent", agent_name)

    def custom_prompt(state):
        # stateè¿™é‡Œæ˜¯æ¶ˆæ¯é›†åˆå¯¹è±¡æˆ–ç±»ä¼¼ç»“æ„
        print("Prompt received state:", state)

        messages = getattr(state, "messages", [])
        merged_state_vars = extract_state_from_messages(messages)
        print("merged state vars", merged_state_vars)

        base_state_dict = {}
        if hasattr(state, "dict"):
            base_state_dict = state.dict(exclude={"messages"}, exclude_none=True)
        elif isinstance(state, dict):
            base_state_dict = {k: v for k, v in state.items() if k != "messages"}

        # merged_state_vars ä¼˜å…ˆè¦†ç›– base_state_dict
        print("base state dict", base_state_dict)
        prompt_vars = {**base_state_dict, **merged_state_vars, "messages": messages}
        print("prompt vars", prompt_vars)
        return apply_prompt_template(prompt_template, state)

    agent = create_react_agent(
        name=agent_name,
        model=get_llm_by_type(AGENT_LLM_MAP[agent_type]),
        tools=tools,
        prompt=custom_prompt,
    )

    def runnable_lambda_fn(input_dict):
        print("RunnableLambda input_dict:", input_dict)
        return {
            **input_dict,
            "messages": input_dict.get("messages", []),
            "question": input_dict.get("question", ""),
            "data_info": input_dict.get("data_info", ""),
        }

    return RunnableLambda(runnable_lambda_fn) | agent
'''

'''
from langgraph.prebuilt import create_react_agent
from langchain_core.runnables import RunnableLambda
from .state import GraphState
from .llm import get_llm_by_type
from .agent import AGENT_LLM_MAP
from .load_template import apply_prompt_template
from langchain_core.messages import BaseMessage

def create_agent(agent_name: str, agent_type: str, tools: list, prompt_template: str):
    print("Creating agent:", agent_name)

    
    def custom_prompt(state):
        print("Prompt received GraphState:", state)

        if isinstance(state, dict):
        # ğŸ‘‡ å°† messages ä¸­çš„ BaseMessage å¯¹è±¡è½¬ä¸º dict
            if "messages" in state and isinstance(state["messages"], list):
                new_messages = []
                for msg in state["messages"]:
                    if isinstance(msg, BaseMessage):
                        new_messages.append(msg.model_dump())  # æˆ–è€… msg.model_dump() å–å†³äºä½ ç”¨çš„æ˜¯å“ªä¸ªåº“
                    elif isinstance(msg, dict):
                        new_messages.append(msg)
                    else:
                        raise ValueError(f"Unsupported message type: {type(msg)}")
                state["messages"] = new_messages
            state = GraphState(**state)

        # æå–æ‰å¹³åŒ–å­—æ®µç”¨äº prompt æ¸²æŸ“
        prompt_vars = state.model_dump(exclude_none=True)
        print("Prompt vars (flattened state):", prompt_vars)

        return apply_prompt_template(prompt_template, prompt_vars)


    agent = create_react_agent(
        name=agent_name,
        model=get_llm_by_type(AGENT_LLM_MAP[agent_type]),
        tools=tools,
        prompt=custom_prompt,
    )

    # ç®€å• passthroughï¼šGraphState -> GraphState dictï¼ˆç”¨äºåç»­èŠ‚ç‚¹å¤„ç†ï¼‰
    def passthrough(state_dict: dict):
        print("RunnableLambda passthrough input:", state_dict)
        return state_dict

    return RunnableLambda(passthrough) | agent
'''